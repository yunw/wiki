<html>
<head>
<link rel="Stylesheet" type="text/css" href="style.css" />
<title>wget</title>
<meta http-equiv="Content-Type" content="text/html; charset=" />
</head>
<body>

<h2 id="toc_0.1">wget下载整个网站</h2>

<p>
可以使用下面的命令 wget -r -p -k -np <a href="http://hi.baidu.com/phps">http://hi.baidu.com/phps</a> , -r 表示递归下载,会下载所有的链接,不过要注意的是,不要单独使用这个参数,因为如果你要下载的网站也有别的网站的链接,wget也会把别的网站的东西下载 下来,由于互联网的特性,很有可能你会把整个互联网给下载下来 --,所以要加上 -np这个参数,表示不下载别的站点的链接. -k表示将下载的网页里的链接修改为本地链接.-p获得所以显示网页所需的元素,比如图片什么的.
另外还有其他的一些参数可以使用:
</p>

<p>
-c表示断点续传
</p>

<p>
-t 100表示重试100次,-t 0表示无穷次重试
</p>

<p>
另外可以将要下载的url写到一个文件中,每个url一行,使用这样的命令 wget -i download.txt.
</p>

<p>
--reject=avi,rmvb 表示不下载avi,rmvb的文件,--accept=jpg,jpeg,表示只下载jpg,jpeg的文件.
</p>

<p>
可以在用户目录下建立一个.wgetrc的文件(windows里面好像不能直接建立这样的文件,windows会认为没有文件名--),里面写上 http-proxy = 123.456.78.9:80,然后在加上参数 --proxy=on,如果需要密码,再加上下面的参数 --proxy-user=username, --proxy-passwd=password
</p>

</body>
</html>
